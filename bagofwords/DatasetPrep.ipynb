{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlines = open(\"../data/bagofwords/trainDataSeq2seq.tsv\").readlines()\n",
    "testlines = open(\"../data/bagofwords/testDataSeq2seq.tsv\").readlines()\n",
    "vallines = open(\"../data/bagofwords/devDataSeq2seq.tsv\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "row=trainlines[0].strip().split(\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If a snow storm is coming you should Buy supplies @@SEP@@ preparing for a storm requires predicting the occurrence of that storm',\n",
       " 'Buy supplies is of preparing for a storm']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If a snow storm is coming you should Buy supplies  preparing for a storm requires predicting the occurrence of that storm',\n",
       " 'Buy supplies is of preparing for a storm']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row[0]=row[0].replace(\"@@SEP@@\",\"\")\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 11\n"
     ]
    }
   ],
   "source": [
    "possible = set([word for word in row[0].split(\" \")])\n",
    "allowed = set([word for word in row[1].split(\" \")])\n",
    "notallowed = possible-allowed\n",
    "print(len(allowed),len(notallowed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=[]\n",
    "possible,allowed,notallowed\n",
    "dataset = []\n",
    "for word in allowed:\n",
    "    dataset.append([row[0],word,1])\n",
    "for word in notallowed:\n",
    "    dataset.append([row[1],word,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['If a snow storm is coming you should Buy supplies  preparing for a storm requires predicting the occurrence of that storm',\n",
       "  'of',\n",
       "  1],\n",
       " ['If a snow storm is coming you should Buy supplies  preparing for a storm requires predicting the occurrence of that storm',\n",
       "  'preparing',\n",
       "  1],\n",
       " ['If a snow storm is coming you should Buy supplies  preparing for a storm requires predicting the occurrence of that storm',\n",
       "  'supplies',\n",
       "  1],\n",
       " ['If a snow storm is coming you should Buy supplies  preparing for a storm requires predicting the occurrence of that storm',\n",
       "  'is',\n",
       "  1],\n",
       " ['If a snow storm is coming you should Buy supplies  preparing for a storm requires predicting the occurrence of that storm',\n",
       "  'storm',\n",
       "  1],\n",
       " ['If a snow storm is coming you should Buy supplies  preparing for a storm requires predicting the occurrence of that storm',\n",
       "  'a',\n",
       "  1],\n",
       " ['If a snow storm is coming you should Buy supplies  preparing for a storm requires predicting the occurrence of that storm',\n",
       "  'Buy',\n",
       "  1],\n",
       " ['If a snow storm is coming you should Buy supplies  preparing for a storm requires predicting the occurrence of that storm',\n",
       "  'for',\n",
       "  1],\n",
       " ['Buy supplies is of preparing for a storm', '', 0],\n",
       " ['Buy supplies is of preparing for a storm', 'predicting', 0],\n",
       " ['Buy supplies is of preparing for a storm', 'If', 0],\n",
       " ['Buy supplies is of preparing for a storm', 'you', 0],\n",
       " ['Buy supplies is of preparing for a storm', 'that', 0],\n",
       " ['Buy supplies is of preparing for a storm', 'should', 0],\n",
       " ['Buy supplies is of preparing for a storm', 'snow', 0],\n",
       " ['Buy supplies is of preparing for a storm', 'coming', 0],\n",
       " ['Buy supplies is of preparing for a storm', 'the', 0],\n",
       " ['Buy supplies is of preparing for a storm', 'requires', 0],\n",
       " ['Buy supplies is of preparing for a storm', 'occurrence', 0]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "stpwords = set(stopwords.words('english'))\n",
    "puncs = set([\".\",\"'\",\"'s'\",\",\"])\n",
    "removewords = stpwords.union(puncs)\n",
    "def create_dataset_bagofwords(lines,fname):\n",
    "    outfd = open(\"../data/bagofwords/\"+fname,\"w+\")\n",
    "    all_max = -1\n",
    "    wordmap = {}\n",
    "    posdataset = []\n",
    "    negdataset = []\n",
    "    for line in lines:\n",
    "        row=line.strip().split(\"\\t\")\n",
    "        row[0]=row[0].replace(\"@@SEP@@\",\".\")\n",
    "        possible = set([word for word in row[0].split(\" \")])\n",
    "        allowed = set([word for word in row[1].split(\" \")])\n",
    "        tnotallowed = possible-allowed\n",
    "        tallowed = allowed  \n",
    "        allowed = []\n",
    "        notallowed = []\n",
    "        for word in tallowed:\n",
    "            if word.lower() not in removewords:\n",
    "                allowed.append(word)\n",
    "        for word in tnotallowed:\n",
    "            if word.lower() not in removewords:\n",
    "                notallowed.append(word)\n",
    "        max_len = -1\n",
    "        for word in allowed:\n",
    "            if word ==\"\":\n",
    "                continue\n",
    "            if word+\"1\" not in wordmap:\n",
    "                wordmap[word+\"1\"]=1\n",
    "            else:\n",
    "                wordmap[word+\"1\"]+=1\n",
    "            max_len = max(max_len,len(row[0].split(\" \"))+1)\n",
    "            posdataset.append([row[0],word,1])\n",
    "        for word in notallowed:\n",
    "            if word ==\"\":\n",
    "                continue\n",
    "            if word+\"0\" not in wordmap:\n",
    "                wordmap[word+\"0\"]=1\n",
    "            else:\n",
    "                wordmap[word+\"0\"]+=1\n",
    "            max_len = max(max_len,len(row[0].split(\" \"))+1)\n",
    "            negdataset.append([row[0],word,0])\n",
    "        all_max = max(max_len,all_max)\n",
    "\n",
    "        \n",
    "    len_pos = len(posdataset)\n",
    "    len_neg = len(negdataset)\n",
    "    final_dataset = []\n",
    "    for dataset in [posdataset,negdataset]:\n",
    "        for row in dataset:\n",
    "            final_dataset.append(row)\n",
    "    if len_pos>len_neg:\n",
    "        diff = len_pos-len_neg\n",
    "        for idx in range(0,diff):\n",
    "            final_dataset.append(negdataset[random.randint(0,len_neg-1)])\n",
    "    elif len_neg > len_pos:\n",
    "        diff = len_neg-len_pos\n",
    "        for idx in range(0,diff):\n",
    "            final_dataset.append(posdataset[random.randint(0,len_pos-1)])\n",
    "\n",
    "    for row in final_dataset:\n",
    "        outfd.write(\"%s\\t%s\\t%d\\n\"%(row[0],row[1],row[2]))\n",
    "        \n",
    "    outfd.close()\n",
    "    print(all_max)\n",
    "    return wordmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "wordmap =  create_dataset_bagofwords(trainlines,\"train.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "w1= create_dataset_bagofwords(testlines,\"test.tsv\")\n",
    "w2= create_dataset_bagofwords(vallines,\"val.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_x = sorted(wordmap.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('day0', 60),\n",
       " ('winter0', 60),\n",
       " ('get0', 60),\n",
       " ('available0', 61),\n",
       " ('surface0', 61),\n",
       " ('needs0', 61),\n",
       " ('survival0', 61),\n",
       " ('around0', 62),\n",
       " ('see0', 62),\n",
       " ('humans0', 63),\n",
       " ('like0', 63),\n",
       " ('temperature0', 63),\n",
       " ('sometimes0', 63),\n",
       " ('nutrients0', 63),\n",
       " ('become0', 64),\n",
       " ('moving0', 64),\n",
       " ('two0', 65),\n",
       " ('organisms0', 66),\n",
       " ('produce0', 66),\n",
       " ('air0', 66),\n",
       " ('process0', 66),\n",
       " ('place0', 66),\n",
       " ('resource0', 67),\n",
       " ('order0', 67),\n",
       " ('found0', 67),\n",
       " ('wants0', 69),\n",
       " ('large0', 69),\n",
       " ('area0', 70),\n",
       " ('sunlight0', 70),\n",
       " ('grow0', 70),\n",
       " ('plant1', 73),\n",
       " ('would1', 73),\n",
       " ('positive0', 73),\n",
       " ('plants1', 75),\n",
       " ('move0', 76),\n",
       " ('sun1', 76),\n",
       " ('changes0', 76),\n",
       " ('using0', 77),\n",
       " ('electrical0', 78),\n",
       " ('objects0', 78),\n",
       " ('electricity0', 78),\n",
       " ('another0', 79),\n",
       " ('find0', 79),\n",
       " ('might0', 79),\n",
       " ('new0', 80),\n",
       " ('heat1', 85),\n",
       " ('time0', 88),\n",
       " ('one0', 90),\n",
       " ('make0', 91),\n",
       " ('animals1', 91),\n",
       " ('change0', 94),\n",
       " ('occurs0', 94),\n",
       " ('usually0', 96),\n",
       " ('things0', 97),\n",
       " ('energy1', 97),\n",
       " ('formed0', 98),\n",
       " ('plants0', 98),\n",
       " ('could0', 99),\n",
       " ('light1', 101),\n",
       " ('decreases0', 104),\n",
       " ('thing0', 106),\n",
       " ('living0', 108),\n",
       " ('means0', 109),\n",
       " ('use0', 109),\n",
       " ('decrease0', 112),\n",
       " ('organism0', 115),\n",
       " ('impact0', 116),\n",
       " ('light0', 117),\n",
       " ('food1', 120),\n",
       " ('live0', 120),\n",
       " ('Earth0', 123),\n",
       " ('body0', 125),\n",
       " ('contains0', 126),\n",
       " ('plant0', 126),\n",
       " ('eat0', 133),\n",
       " ('made0', 135),\n",
       " ('heat0', 142),\n",
       " ('food0', 159),\n",
       " ('increase0', 162),\n",
       " (\"'s0\", 164),\n",
       " ('something0', 166),\n",
       " ('animal0', 177),\n",
       " ('requires0', 179),\n",
       " ('may0', 180),\n",
       " ('energy0', 180),\n",
       " ('amount0', 181),\n",
       " ('animals0', 201),\n",
       " ('water0', 207),\n",
       " ('cause0', 211),\n",
       " ('increases0', 214),\n",
       " ('object0', 219),\n",
       " ('water1', 222),\n",
       " ('environment0', 232),\n",
       " ('source0', 238),\n",
       " ('person0', 245),\n",
       " ('example0', 255),\n",
       " ('would0', 297),\n",
       " ('causes0', 305),\n",
       " ('likely0', 309),\n",
       " ('used0', 391)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_x[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w2', 'word'}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([\"word\"]).union(set([\"w2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare scoring for bag-of-words Model\n",
    "import csv\n",
    "def create_score_dataset_for_bow(fname):\n",
    "    lines = open(\"../data/with-genfact2/\"+fname).readlines()\n",
    "    ofd = open(\"../data/with-genfact2/\"+fname+\"-forscore.tsv\",\"w+\")\n",
    "    csvout = csv.writer(ofd,delimiter='\\t')\n",
    "    for line in lines:\n",
    "        row = line.strip().split(\"\\t\")\n",
    "        base = row[0]\n",
    "        hyp = row[1]\n",
    "        fact = row[2]\n",
    "        hyp_words = set(hyp.split(\" \"))\n",
    "        fact_words = set(fact.split(\" \"))\n",
    "        tall_words = hyp_words.union(fact_words)\n",
    "        allowed=[]\n",
    "        for word in tall_words:\n",
    "            if word.lower() not in removewords:\n",
    "                allowed.append(word)\n",
    "        row.append(\"temp\")\n",
    "        for index,word in enumerate(allowed):\n",
    "            idx = base+\":\"+str(index)\n",
    "            row[0]=idx\n",
    "            row[-1]= word\n",
    "            csvout.writerow(row)\n",
    "    ofd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_score_dataset_for_bow(\"test-trainedwithFact2.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_score_dataset_for_bow(\"val-trainedwithFact2.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_score_dataset_for_bow(\"top10-merged-testwithFact2.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_score_dataset_for_bow(\"top10-merged-valwithFact2.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_post_scored_dataset_for_ir(fname):\n",
    "    trainlines = open(\"../data/with-genfact2/\"+fname+\"-forscore.tsv\").readlines()\n",
    "    scorelines = open(\"../data/with-genfact2/\"+fname+\"-forscore.tsv-score.tsv\").readlines()\n",
    "    ofd = open(\"../data/with-genfact2/bowscored/\"+fname,\"w+\")\n",
    "    csvout = csv.writer(ofd,delimiter='\\t')\n",
    "    \n",
    "    scoredict = {}\n",
    "    for line in scorelines:\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        scoredict[line[0]]=float(line[1])\n",
    "    \n",
    "    rows= {}\n",
    "    for line in trainlines:\n",
    "        row = line.strip().split(\"\\t\")\n",
    "        base = row[0]\n",
    "        hyp = row[1]\n",
    "        fact = row[2]\n",
    "        label = row[3]\n",
    "        fscore = row[4]\n",
    "        target = row[5]\n",
    "        word = row[-1]\n",
    "        score = scoredict[base]\n",
    "        if score >= 0.4:\n",
    "            idsplit = base.split(\":\")\n",
    "            idx = idsplit[0]+\":\"+idsplit[1]+\":\"+idsplit[2]\n",
    "            if idx not in rows:\n",
    "                rows[idx]=[idx,hyp,fact,label,fscore,word]\n",
    "            else:\n",
    "                words = rows[idx][-1]\n",
    "                words += \" \" + word\n",
    "                rows[idx][-1] = words           \n",
    "    \n",
    "    for k,row in rows.items():\n",
    "        csvout.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_post_scored_dataset_for_ir(\"test-trainedwithFact2.tsv\")\n",
    "create_post_scored_dataset_for_ir(\"val-trainedwithFact2.tsv\")\n",
    "create_post_scored_dataset_for_ir(\"top10-merged-testwithFact2.tsv\")\n",
    "create_post_scored_dataset_for_ir(\"top10-merged-valwithFact2.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'predator predotor lion lunch'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(list(set(\"predotor Predator lion lunch Lion\".lower().split(\" \"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
