
local word_embedding_dim = 256;
local char_embedding_dim = 256;
local embedding_dim = word_embedding_dim + 2*char_embedding_dim;
local hidden_dim = 100;
local epochs = 30;
local patience = 10;
local batch_size = 50;
local lr = 0.01;
{
  "dataset_reader": {
    "type": "seq2seq",
    "source_tokenizer": {
      "type": "word",
    },
    "target_tokenizer": {
      "type": "word"
    },
    "source_token_indexers": {
      "tokens": {
        "type": "single_id",
        "namespace": "source_tokens"
      },
        "token_characters":{"type": "characters"}
    },
    "target_token_indexers": {
      "tokens": {
        "namespace": "target_tokens"
      }
    }
  },
  "train_data_path": "processedData/Original_Fact2_Aug/trainDataSeq2seqAug.tsv",
  "validation_data_path": "processedData/Original_Fact2_Aug/devDataSeq2seqAug.tsv",
  "model": {
    "type": "simple_seq2seq",
    "source_embedder": {
      "token_embedders": {
        "tokens": {
          "type": "embedding",
          "vocab_namespace": "source_tokens",
          "embedding_dim": word_embedding_dim,
          "trainable": true
        },
        "token_characters":{
            "type": "character_encoding",
            "embedding": {
                "embedding_dim": char_embedding_dim
                },
            "encoder":{
            "type": "lstm",
          "input_size": char_embedding_dim,
          "hidden_size": char_embedding_dim,
          "num_layers": 1,
          "dropout": 0,
          "bidirectional": true
            }
        }
      }
    },
    "encoder": {
      "type": "lstm",
      "input_size": embedding_dim,
      "hidden_size": hidden_dim,
      "num_layers": 2
    },
    "max_decoding_steps": 20,
    "target_embedding_dim": 30,
    "target_namespace": "target_tokens",
    "attention": {
      "type": "dot_product"
    },
    "beam_size": 3
  },
  "iterator": {
    "type": "bucket",
    "padding_noise": 0.0,
    "batch_size" : batch_size,
    "sorting_keys": [["source_tokens", "num_tokens"]]
  },
  "trainer": {
    "num_epochs": epochs,
    "patience": 10,
    "cuda_device": 0,
    "optimizer": {
      "type": "adam",
      "lr": lr
    }
  }
}
